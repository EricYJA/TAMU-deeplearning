{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d0a7fc",
   "metadata": {},
   "source": [
    "## CSCE636 Deep Learning - Dr. Anxiao Jiang\n",
    "- Changcheng Yuan\n",
    "- UIN: 53500517"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273513c",
   "metadata": {},
   "source": [
    "Check out the Jupyter notebook for Chapter 13 at https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter13_timeseries-forecasting.ipynb . It has tried 6 methods for the temperature prediction problem: Try 1 (A common-sense, non-machine learning baseline method), Try 2 (A fully connected neural network), Try 3 (1-d convolutional neural networks), Try 4 (LSTM with recurrent dropout), Try 5 (stacking RNN layers), Try 6 (Bidirectional RNN).\n",
    "\n",
    "Your task: use the above 6 methods to predict the temperature in 48 hours (instead of 24 hours). In the Jupyter notebook, include your code as well as the performance of the 6 methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0937d89",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd ../data/chapter13 && wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "!cd ../data/chapter13 && unzip jena_climate_2009_2016.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f980bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs found: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Built with CUDA: True\n",
      "GPU available: True\n",
      "Default GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs found:\", gpus)\n",
    "\n",
    "# Quick check\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available:\", len(gpus) > 0)\n",
    "# Or get the default device name\n",
    "print(\"Default GPU device:\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4f2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../data/chapter13/jena_climate_2009_2016.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3ae7b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 420451 timesteps across 14 sensors\n",
      "Train/val/test samples: 210225/105112/105114\n",
      "Forecast horizon: 48 hours (delay steps: 1002)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "with open(datapath) as f:\n",
    "    data = f.read().strip().split(\"\\n\")\n",
    "\n",
    "header = data[0].split(\",\")\n",
    "lines = data[1:]\n",
    "num_features = len(header) - 1\n",
    "\n",
    "raw_data = np.zeros((len(lines), num_features), dtype=np.float32)\n",
    "temperature = np.zeros((len(lines),), dtype=np.float32)\n",
    "for idx, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]\n",
    "    temperature[idx] = values[1]\n",
    "    raw_data[idx] = values\n",
    "\n",
    "num_train_samples = int(0.5 * len(raw_data))\n",
    "num_val_samples = int(0.25 * len(raw_data))\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
    "\n",
    "mean = raw_data[:num_train_samples].mean(axis=0)\n",
    "std = raw_data[:num_train_samples].std(axis=0)\n",
    "std[std == 0] = 1.0\n",
    "raw_data = (raw_data - mean) / std\n",
    "\n",
    "sampling_rate = 6\n",
    "time_steps = 120\n",
    "forecast_horizon_hours = 48\n",
    "# Offset between the last input time step and the prediction target\n",
    "sequence_delay = sampling_rate * (time_steps + forecast_horizon_hours - 1)\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-sequence_delay],\n",
    "    targets=temperature[sequence_delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=time_steps,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    start_index=0,\n",
    "    end_index=num_train_samples,\n",
    ")\n",
    "\n",
    "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-sequence_delay],\n",
    "    targets=temperature[sequence_delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=time_steps,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    start_index=num_train_samples,\n",
    "    end_index=num_train_samples + num_val_samples,\n",
    ")\n",
    "\n",
    "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    raw_data[:-sequence_delay],\n",
    "    targets=temperature[sequence_delay:],\n",
    "    sampling_rate=sampling_rate,\n",
    "    sequence_length=time_steps,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    start_index=num_train_samples + num_val_samples,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(lines)} timesteps across {num_features} sensors\")\n",
    "print(f\"Train/val/test samples: {num_train_samples}/{num_val_samples}/{num_test_samples}\")\n",
    "print(f\"Forecast horizon: {forecast_horizon_hours} hours (delay steps: {sequence_delay})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe61519",
   "metadata": {},
   "source": [
    "### Try 1\n",
    "\n",
    "Simple non-linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66133f4b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples used: 60000\n",
      "Train MAE: 3.32 °C\n",
      "Validation MAE: 3.06 °C\n",
      "Test MAE: 3.28 °C\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "history_hours = 48\n",
    "max_train_samples = 60000\n",
    "\n",
    "\n",
    "def collect_numpy(dataset, limit=None):\n",
    "    features, targets = [], []\n",
    "    seen = 0\n",
    "    for samples, target in dataset:\n",
    "        samples_np = np.asarray(samples)\n",
    "        target_np = np.asarray(target)\n",
    "        temp_history = samples_np[:, -history_hours:, 1]\n",
    "        features.append(temp_history)\n",
    "        targets.append(target_np)\n",
    "        seen += len(target_np)\n",
    "        if limit and seen >= limit:\n",
    "            break\n",
    "    X = np.concatenate(features, axis=0)\n",
    "    y = np.concatenate(targets, axis=0)\n",
    "    if limit:\n",
    "        X = X[:limit]\n",
    "        y = y[:limit]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def evaluate_mae(model, dataset):\n",
    "    total_abs_err = 0.0\n",
    "    samples_seen = 0\n",
    "    for samples, target in dataset:\n",
    "        features = np.asarray(samples)[:, -history_hours:, 1]\n",
    "        preds = model.predict(features)\n",
    "        target_np = np.asarray(target)\n",
    "        total_abs_err += np.sum(np.abs(preds - target_np))\n",
    "        samples_seen += len(target_np)\n",
    "    return total_abs_err / samples_seen\n",
    "\n",
    "\n",
    "train_X, train_y = collect_numpy(train_dataset, limit=max_train_samples)\n",
    "\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", Ridge(alpha=100.0)),\n",
    "    ]\n",
    ")\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "train_mae = mean_absolute_error(train_y, model.predict(train_X))\n",
    "val_mae = evaluate_mae(model, val_dataset)\n",
    "test_mae = evaluate_mae(model, test_dataset)\n",
    "\n",
    "print(f\"Train samples used: {len(train_X)}\")\n",
    "print(f\"Train MAE: {train_mae:.2f} °C\")\n",
    "print(f\"Validation MAE: {val_mae:.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try2-md",
   "metadata": {},
   "source": [
    "### Try 2\n",
    "\n",
    "Simple dense network using MSE for training and MAE for human readable result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "try2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760975739.852349  159012 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819/819 [==============================] - 4s 3ms/step - loss: 8.7333 - mse: 8.7333 - mae: 2.1585 - val_loss: 23.5491 - val_mse: 23.5491 - val_mae: 3.7951\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 4s 3ms/step - loss: 8.7333 - mse: 8.7333 - mae: 2.1585 - val_loss: 23.5491 - val_mse: 23.5491 - val_mae: 3.7951\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 1.5561 - mse: 1.5561 - mae: 0.9549 - val_loss: 21.5729 - val_mse: 21.5729 - val_mae: 3.6621\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 1.5561 - mse: 1.5561 - mae: 0.9549 - val_loss: 21.5729 - val_mse: 21.5729 - val_mae: 3.6621\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.9221 - mse: 0.9221 - mae: 0.7417 - val_loss: 22.0839 - val_mse: 22.0839 - val_mae: 3.7007\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.9221 - mse: 0.9221 - mae: 0.7417 - val_loss: 22.0839 - val_mse: 22.0839 - val_mae: 3.7007\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.7774 - mse: 0.7774 - mae: 0.6807 - val_loss: 21.1179 - val_mse: 21.1179 - val_mae: 3.6330\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.7774 - mse: 0.7774 - mae: 0.6807 - val_loss: 21.1179 - val_mse: 21.1179 - val_mae: 3.6330\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 0.6669 - mse: 0.6669 - mae: 0.6312 - val_loss: 20.8940 - val_mse: 20.8940 - val_mae: 3.6079\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 0.6669 - mse: 0.6669 - mae: 0.6312 - val_loss: 20.8940 - val_mse: 20.8940 - val_mae: 3.6079\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.6301 - mse: 0.6301 - mae: 0.6130 - val_loss: 19.9695 - val_mse: 19.9695 - val_mae: 3.5230\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.6301 - mse: 0.6301 - mae: 0.6130 - val_loss: 19.9695 - val_mse: 19.9695 - val_mae: 3.5230\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5816 - mse: 0.5816 - mae: 0.5903 - val_loss: 20.4368 - val_mse: 20.4368 - val_mae: 3.5769\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5816 - mse: 0.5816 - mae: 0.5903 - val_loss: 20.4368 - val_mse: 20.4368 - val_mae: 3.5769\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5132 - mse: 0.5132 - mae: 0.5533 - val_loss: 19.7403 - val_mse: 19.7403 - val_mae: 3.5076\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5132 - mse: 0.5132 - mae: 0.5533 - val_loss: 19.7403 - val_mse: 19.7403 - val_mae: 3.5076\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5188 - mse: 0.5188 - mae: 0.5563 - val_loss: 20.5611 - val_mse: 20.5611 - val_mae: 3.5775\n",
      "Epoch 10/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.5188 - mse: 0.5188 - mae: 0.5563 - val_loss: 20.5611 - val_mse: 20.5611 - val_mae: 3.5775\n",
      "Epoch 10/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 0.4907 - mse: 0.4907 - mae: 0.5400 - val_loss: 19.7154 - val_mse: 19.7154 - val_mae: 3.5134\n",
      "Epoch 11/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 0.4907 - mse: 0.4907 - mae: 0.5400 - val_loss: 19.7154 - val_mse: 19.7154 - val_mae: 3.5134\n",
      "Epoch 11/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.4389 - mse: 0.4389 - mae: 0.5122 - val_loss: 20.0990 - val_mse: 20.0990 - val_mae: 3.5392\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 0.4389 - mse: 0.4389 - mae: 0.5122 - val_loss: 20.0990 - val_mse: 20.0990 - val_mae: 3.5392\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 20.0430 - mse: 20.0430 - mae: 3.5808\n",
      "Train MAE: 0.51 °C\n",
      "Validation MAE: 3.54 °C\n",
      "Test MAE: 3.58 °C\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 20.0430 - mse: 20.0430 - mae: 3.5808\n",
      "Train MAE: 0.51 °C\n",
      "Validation MAE: 3.54 °C\n",
      "Test MAE: 3.58 °C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "\n",
    "def flatten_dataset(dataset):\n",
    "    return dataset.map(lambda x, y: (tf.reshape(x, (tf.shape(x)[0], -1)), y))\n",
    "\n",
    "\n",
    "flattened_train = flatten_dataset(train_dataset).prefetch(tf.data.AUTOTUNE)\n",
    "flattened_val = flatten_dataset(val_dataset).prefetch(tf.data.AUTOTUNE)\n",
    "flattened_test = flatten_dataset(test_dataset).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "input_dim = time_steps * num_features\n",
    "\n",
    "dense_model = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mse\", \"mae\"],\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = dense_model.fit(\n",
    "    flattened_train,\n",
    "    validation_data=flattened_val,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "test_loss, test_mse, test_mae = dense_model.evaluate(flattened_test)\n",
    "print(f\"Train MAE: {history.history['mae'][-1]:.2f} °C\")\n",
    "print(f\"Validation MAE: {history.history['val_mae'][-1]:.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try3-md",
   "metadata": {},
   "source": [
    "### Try 3\n",
    "\n",
    "1D convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "try3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "819/819 [==============================] - 4s 4ms/step - loss: 26.7248 - mae: 4.0525 - val_loss: 21.7124 - val_mae: 3.6593\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 4s 4ms/step - loss: 26.7248 - mae: 4.0525 - val_loss: 21.7124 - val_mae: 3.6593\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 17.2363 - mae: 3.3040 - val_loss: 22.9234 - val_mae: 3.7617\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 17.2363 - mae: 3.3040 - val_loss: 22.9234 - val_mae: 3.7617\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 15.0088 - mae: 3.0766 - val_loss: 23.2874 - val_mae: 3.7093\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 15.0088 - mae: 3.0766 - val_loss: 23.2874 - val_mae: 3.7093\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 13.7035 - mae: 2.9317 - val_loss: 23.3120 - val_mae: 3.8008\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 13.7035 - mae: 2.9317 - val_loss: 23.3120 - val_mae: 3.8008\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 12.7704 - mae: 2.8225 - val_loss: 20.8773 - val_mae: 3.6062\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 12.7704 - mae: 2.8225 - val_loss: 20.8773 - val_mae: 3.6062\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 12.0872 - mae: 2.7393 - val_loss: 21.4753 - val_mae: 3.6310\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 12.0872 - mae: 2.7393 - val_loss: 21.4753 - val_mae: 3.6310\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 11.5291 - mae: 2.6740 - val_loss: 22.7263 - val_mae: 3.7289\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 11.5291 - mae: 2.6740 - val_loss: 22.7263 - val_mae: 3.7289\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 11.0612 - mae: 2.6199 - val_loss: 22.7498 - val_mae: 3.7557\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 11.0612 - mae: 2.6199 - val_loss: 22.7498 - val_mae: 3.7557\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 10.6719 - mae: 2.5716 - val_loss: 22.5056 - val_mae: 3.7476\n",
      "Epoch 10/20\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 10.6719 - mae: 2.5716 - val_loss: 22.5056 - val_mae: 3.7476\n",
      "Epoch 10/20\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 10.3544 - mae: 2.5321 - val_loss: 24.3774 - val_mae: 3.8577\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 10.3544 - mae: 2.5321 - val_loss: 24.3774 - val_mae: 3.8577\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 22.0820 - mae: 3.7204\n",
      "Train MAE: 2.53 °C\n",
      "Validation MAE: 3.86 °C\n",
      "Test MAE: 3.72 °C\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 22.0820 - mae: 3.7204\n",
      "Train MAE: 2.53 °C\n",
      "Validation MAE: 3.86 °C\n",
      "Test MAE: 3.72 °C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, callbacks, layers, optimizers\n",
    "\n",
    "conv_train = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "conv_val = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "conv_test = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "inputs = Input(shape=(time_steps, num_features))\n",
    "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "conv_model = Model(inputs, outputs)\n",
    "\n",
    "conv_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = conv_model.fit(\n",
    "    conv_train,\n",
    "    validation_data=conv_val,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "test_loss, test_mae = conv_model.evaluate(conv_test)\n",
    "print(f\"Train MAE: {history.history['mae'][-1]:.2f} °C\")\n",
    "print(f\"Validation MAE: {history.history['val_mae'][-1]:.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try4-md",
   "metadata": {},
   "source": [
    "### Try 4\n",
    "\n",
    "LSTM with recurrent dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "try4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "819/819 [==============================] - 56s 67ms/step - loss: 35.3035 - mae: 4.4916 - val_loss: 17.2356 - val_mae: 3.2307\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 56s 67ms/step - loss: 35.3035 - mae: 4.4916 - val_loss: 17.2356 - val_mae: 3.2307\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 21.0864 - mae: 3.5989 - val_loss: 15.9922 - val_mae: 3.1183\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 21.0864 - mae: 3.5989 - val_loss: 15.9922 - val_mae: 3.1183\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 19.9845 - mae: 3.5062 - val_loss: 15.8539 - val_mae: 3.1060\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 19.9845 - mae: 3.5062 - val_loss: 15.8539 - val_mae: 3.1060\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 18.8643 - mae: 3.4021 - val_loss: 15.5402 - val_mae: 3.0776\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 18.8643 - mae: 3.4021 - val_loss: 15.5402 - val_mae: 3.0776\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 18.2988 - mae: 3.3445 - val_loss: 15.5770 - val_mae: 3.0922\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 18.2988 - mae: 3.3445 - val_loss: 15.5770 - val_mae: 3.0922\n",
      "Epoch 6/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 17.6314 - mae: 3.2802 - val_loss: 15.4021 - val_mae: 3.0725\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 17.6314 - mae: 3.2802 - val_loss: 15.4021 - val_mae: 3.0725\n",
      "Epoch 7/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 17.1580 - mae: 3.2363 - val_loss: 15.1410 - val_mae: 3.0518\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 17.1580 - mae: 3.2363 - val_loss: 15.1410 - val_mae: 3.0518\n",
      "Epoch 8/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 16.9492 - mae: 3.2148 - val_loss: 15.8922 - val_mae: 3.1056\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 16.9492 - mae: 3.2148 - val_loss: 15.8922 - val_mae: 3.1056\n",
      "Epoch 9/20\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 16.1298 - mae: 3.1248 - val_loss: 15.9763 - val_mae: 3.1246\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 16.1298 - mae: 3.1248 - val_loss: 15.9763 - val_mae: 3.1246\n",
      "404/404 [==============================] - 6s 14ms/step - loss: 17.5953 - mae: 3.3301\n",
      "Train MAE: 3.12 °C\n",
      "Validation MAE: 3.12 °C\n",
      "Test MAE: 3.33 °C\n",
      "404/404 [==============================] - 6s 14ms/step - loss: 17.5953 - mae: 3.3301\n",
      "Train MAE: 3.12 °C\n",
      "Validation MAE: 3.12 °C\n",
      "Test MAE: 3.33 °C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, callbacks, layers, optimizers\n",
    "\n",
    "lstm_train = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "lstm_val = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "lstm_test = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "debug_batches = None # disable debug\n",
    "if debug_batches:\n",
    "    lstm_train = lstm_train.take(debug_batches)\n",
    "    lstm_val = lstm_val.take(max(1, debug_batches // 4))\n",
    "\n",
    "inputs = Input(shape=(time_steps, num_features))\n",
    "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "lstm_model = Model(inputs, outputs)\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"],\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    lstm_train,\n",
    "    validation_data=lstm_val,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "test_loss, test_mae = lstm_model.evaluate(lstm_test)\n",
    "print(f\"Train MAE: {history.history['mae'][-1]:.2f} °C\")\n",
    "print(f\"Validation MAE: {history.history['val_mae'][-1]:.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fb7dc",
   "metadata": {},
   "source": [
    "### Try 5\n",
    "stacking RNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1bcd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/20\n",
      "819/819 [==============================] - 104s 125ms/step - loss: 33.5239 - mae: 4.3813 - val_loss: 16.9930 - val_mae: 3.1807\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 102s 125ms/step - loss: 19.6760 - mae: 3.4707 - val_loss: 16.1882 - val_mae: 3.1607\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 103s 125ms/step - loss: 18.0445 - mae: 3.3191 - val_loss: 15.9455 - val_mae: 3.1536\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 103s 125ms/step - loss: 16.6902 - mae: 3.1885 - val_loss: 16.8907 - val_mae: 3.2779\n",
      "Epoch 5/20\n",
      "819/819 [==============================] - 102s 125ms/step - loss: 15.3300 - mae: 3.0474 - val_loss: 16.9818 - val_mae: 3.2825\n",
      "404/404 [==============================] - 10s 24ms/step - loss: 17.7145 - mae: 3.3138\n",
      "Training MAE: 3.05 °C\n",
      "Validation MAE: 3.15 °C\n",
      "Test MAE: 3.31 °C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "gru_train = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "gru_val = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "gru_test = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Stack GRU layers with dropout to regularize the recurrent model\n",
    "inputs = keras.Input(shape=(time_steps, num_features))\n",
    "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
    "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "stacked_gru_model = keras.Model(inputs, outputs)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_mae\",\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "stacked_gru_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = stacked_gru_model.fit(\n",
    "    gru_train,\n",
    "    epochs=20,\n",
    "    validation_data=gru_val,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "train_mae = history.history[\"mae\"][-1]\n",
    "val_mae = min(history.history[\"val_mae\"])\n",
    "\n",
    "_, test_mae = stacked_gru_model.evaluate(gru_test)\n",
    "print(f\"Training MAE: {train_mae:.2f} °C\")\n",
    "print(f\"Validation MAE: {val_mae:.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371ae97",
   "metadata": {},
   "source": [
    "### Try 6\n",
    "\n",
    "bi-directional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220af8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "819/819 [==============================] - 7s 7ms/step - loss: 34.1327 - mae: 4.3478 - val_loss: 17.0312 - val_mae: 3.2301\n",
      "Epoch 2/20\n",
      "819/819 [==============================] - 6s 7ms/step - loss: 15.7457 - mae: 3.1380 - val_loss: 16.2235 - val_mae: 3.1491\n",
      "Epoch 3/20\n",
      "819/819 [==============================] - 6s 7ms/step - loss: 14.3200 - mae: 2.9888 - val_loss: 16.2865 - val_mae: 3.1655\n",
      "Epoch 4/20\n",
      "819/819 [==============================] - 6s 7ms/step - loss: 13.7482 - mae: 2.9283 - val_loss: 16.5286 - val_mae: 3.2029\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 18.4855 - mae: 3.4101\n",
      "Train MAE: 2.93 °C\n",
      "Validation MAE: 3.15 °C\n",
      "Test MAE: 3.41 °C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "bidi_train = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "bidi_val = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "bidi_test = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "inputs = keras.Input(shape=(time_steps, num_features))\n",
    "x = layers.Bidirectional(layers.LSTM(16))(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "bidirectional_model = keras.Model(inputs, outputs)\n",
    "\n",
    "bidirectional_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = bidirectional_model.fit(\n",
    "    bidi_train,\n",
    "    epochs=20,\n",
    "    validation_data=bidi_val,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "test_loss, test_mae = bidirectional_model.evaluate(bidi_test)\n",
    "print(f\"Train MAE: {history.history['mae'][-1]:.2f} °C\")\n",
    "print(f\"Validation MAE: {min(history.history['val_mae']):.2f} °C\")\n",
    "print(f\"Test MAE: {test_mae:.2f} °C\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
